# ðŸ† Judge Playbook â€” Agents League Battle #2: Certification Prep System

> Reference set for hackathon judges, interviewers, and technical reviewers.
> Written to answer the hard questions about architecture, algorithms, safety, and production readiness.

---

## Executive Summary

This is a **multi-agent AI system** â€” not a single-prompt LLM call â€” that creates a personalised, adaptive study plan for any Microsoft certification exam. Eight specialised agents collaborate through orchestration patterns found in production AI systems: typed handoffs, human-in-the-loop gates, conditional routing, and cross-cutting responsible AI guardrails.

**What judges / interviewers typically ask:**
1. "Why multi-agent? Couldn't one LLM call do this?" â†’ [see Â§2]
2. "How exactly does the safety layer work?" â†’ [see Â§3]
3. "Walk me through the algorithm for the study plan." â†’ [see Â§4]
4. "How is readiness calculated? Show me the formula." â†’ [see Â§5]
5. "What's the data flow start to finish?" â†’ [see Â§6]
6. "What would production deployment look like?" â†’ [see Â§8]
7. "Why sequential and not parallel agents?" â†’ [see Â§9]

---

## 1. Four Orchestration Patterns â€” In Detail

### Pattern 1: Sequential Pipeline

**What it is:** Agents execute in fixed order, each receiving the fully-validated output of the prior agent as its typed input.

**Why:** The pipeline has hard data dependencies. `StudyPlanAgent` cannot run without a complete `LearnerProfile` because it needs `domain_profiles`, `risk_domains`, `learning_style`, `experience_level`, and `total_budget_hours` â€” all of which are fields on the Pydantic model created by `LearnerProfilingAgent`. There is no way to partially satisfy this.

**Code pattern:**
```python
# streamlit_app.py â€” orchestration sequence
raw: RawStudentInput = collect_intake_form()           # B0
_   = GuardrailsPipeline().check_input(raw)            # G-01..G-05 BLOCK gate
profile: LearnerProfile = LearnerProfilingAgent().run(raw)    # B1 profiler
plan: StudyPlan = StudyPlanAgent().run_with_raw(        # B1.1 plan
    profile, st.session_state.existing_certs
)
path: LearningPath = LearningPathCuratorAgent().run(profile)  # B1.1 path
# ... HITL gate (progress form) ...
readiness: ReadinessAssessment = ProgressAgent().run(snapshot, profile)   # B1.2
# ... HITL gate (quiz submission) ...
result: AssessmentResult = AssessmentAgent().score(answers, assessment)   # B2
rec: CertRecommendation = CertRecommendationAgent().run(result, profile)  # B3
```

### Pattern 2: Typed Handoff

**What it is:** Every inter-agent message is a **Pydantic BaseModel or Python dataclass** â€” never a raw string, dictionary, or JSON blob.

**Why this matters:** In production systems, unstructured handoffs are the #1 source of hard-to-debug failures. Typed contracts enforce: field existence, type correctness, constraint validation (e.g., `confidence: float` = validated âˆˆ [0,1] by guardian G-07), and make the pipeline inspectable without parsing strings.

| From | To | Contract Type |
|------|----|--------------|
| UI form | IntakeAgent | `RawStudentInput` (dataclass) |
| IntakeAgent | ProfilerAgent | `RawStudentInput` (dataclass) |
| ProfilerAgent | StudyPlanAgent | `LearnerProfile` (Pydantic) |
| ProfilerAgent | LearningPathAgent | `LearnerProfile` (Pydantic) |
| Progress form + StudyPlan | ProgressAgent | `ProgressSnapshot` (dataclass) |
| ProgressAgent | (UI readiness display) | `ReadinessAssessment` (dataclass) |
| ProfilerAgent | AssessmentAgent | `LearnerProfile` (Pydantic) |
| AssessmentAgent | CertRecAgent | `AssessmentResult` (dataclass) |

**Pydantic validation example:**
```python
class LearnerProfile(BaseModel):
    domain_profiles: list[DomainProfile]

    @model_validator(mode='after')
    def _validate_domains(self):
        assert len(self.domain_profiles) == 6, "Must have exactly 6 domain profiles"
        for dp in self.domain_profiles:
            assert 0.0 <= dp.confidence <= 1.0, f"Confidence out of range: {dp.domain_id}"
        return self
```

If a profiler returns malformed data (e.g., `confidence = 1.3`), validation fails at the handoff boundary â€” before any downstream agent runs.

### Pattern 3: Human-in-the-Loop (HITL)

**What it is:** Two explicit pause points where the pipeline **stops and waits for real human input** before continuing.

**Why:** Without HITL gates, readiness scores would be computed from fictional data. This is the key ethical requirement: an AI system must not manufacture a "72% ready â€” book your exam!" verdict from no actual study evidence.

**Gate 1 â€” Progress Check-In (between B1.2 and B2):**
```python
# st.session_state.progress_submitted is False until the user fills and submits the form
if not st.session_state.progress_submitted:
    with st.form("progress_form"):
        hours_spent = st.slider("Hours studied this week", 0, 40, 0)
        domain_ratings = {d: st.slider(d, 1, 5, 3) for d in domains}
        practice_score = st.number_input("Last practice exam score", 0, 100, 0)
        if st.form_submit_button("Submit Progress"):
            st.session_state.progress_submitted = True
            # Only NOW does ProgressAgent run
```

**Gate 2 â€” Quiz Submission (between B2 scoring and B3):**
```python
if not st.session_state.quiz_submitted:
    # render each question with radio buttons
    if st.button("Submit Quiz"):
        st.session_state.answers = collected_answers
        st.session_state.quiz_submitted = True
        # Only NOW does AssessmentAgent.score() run
```

### Pattern 4: Conditional Routing with Feedback Loop

**What it is:** After `ReadinessAssessment`, the pipeline branches: GO â†’ booking â†’ CertRecommendation; NOT YET â†’ remediation â†’ back to StudyPlan.

```python
if readiness.exam_go_nogo == "GO":
    render_certification_booking_section()
    run_cert_recommendation_agent()
elif readiness.exam_go_nogo == "CONDITIONAL GO":
    render_gap_analysis()
    offer_targeted_resources()
else:  # "NOT YET"
    render_remediation_plan()
    rebuild_study_plan_for_weak_domains()   # loops back to StudyPlanAgent
```

**Routing thresholds:**
- `readiness_pct >= 70` â†’ **GO** (green, book exam)
- `50 <= readiness_pct < 70` â†’ **CONDITIONAL GO** (amber, targeted review)
- `readiness_pct < 50` â†’ **NOT YET** (red, remediation loop)

---

## 2. Why Multi-Agent? (The "Couldn't one LLM do this?" answer)

A single LLM prompt-chain fails at:

| Requirement | Single LLM | Multi-Agent |
|-------------|-----------|-------------|
| Typed data contracts at every stage | âŒ Raw text | âœ… Pydantic validation |
| 17 independently testable safety rules | âŒ Embedded in prompt | âœ… Separate `guardrails.py` |
| HITL gates that genuinely pause execution | âŒ Simulated | âœ… Real `st.session_state` stops |
| Deterministic quiz sampling (no hallucination) | âŒ LLM can hallucinate | âœ… Rule-based Largest Remainder |
| Persistent session recovery across browser refresh | âŒ Context lost | âœ… SQLite `students` + `profiles` tables |
| Per-agent reasoning traces for audit | âŒ One opaque response | âœ… `AgentStep` log per agent |
| Conditional routing based on computed readiness | âŒ LLM decides (unreliable) | âœ… Deterministic formula |

Each agent also has a **bounded responsibility** â€” any agent can be swapped independently (e.g., replace mock profiler with GPT-4o profiler, or replace rule-based assessment with Azure AI Search question bank) without touching the others.

---

## 3. Guardrail Architecture â€” Deep Dive

### The FaÃ§ade Pattern

`GuardrailsPipeline` wraps every agent boundary. It intercepts inputs and outputs, applies up to 17 rules, and returns a `GuardrailResult` with: `passed` (bool), `blocked` (bool), `violations` (list of `GuardrailViolation`).

```python
@dataclass
class GuardrailViolation:
    code: str        # e.g. "G-04"
    level: str       # "BLOCK" / "WARN" / "INFO"
    message: str     # human-readable description
    field: str       # which field triggered it

@dataclass
class GuardrailResult:
    passed: bool
    blocked: bool
    violations: list[GuardrailViolation]
```

### The 17 Rules Explained

**Input Validation (G-01..G-05):**
- G-01 `BLOCK` â€” `student_name` and `exam_target` non-empty (can't profile without these)
- G-02 `BLOCK` â€” `hours_per_week` âˆˆ [1, 80] (>80 hrs/week â†’ unsafe study load)
- G-03 `BLOCK` â€” `weeks_available` âˆˆ [1, 52] (prevents nonsensical plans)
- G-04 `WARN` â€” `exam_target` not in recognised list (soft warning; system continues but flags unknown exam)
- G-05 `INFO` â€” No background text provided (student may get a less accurate profile; advisory only)

**Profile Integrity (G-06..G-08):**
- G-06 `BLOCK` â€” `domain_profiles` must have exactly 6 items (AI-102 has 6 domains; miscount means profiler failed)
- G-07 `BLOCK` â€” Each `confidence` âˆˆ [0.0, 1.0] (out-of-range confidence scores would corrupt readiness calc)
- G-08 `WARN` â€” `risk_domains` IDs must all be valid domain IDs from the exam registry

**Study Plan Bounds (G-09..G-10):**
- G-09 `BLOCK` â€” No task with `start_week > end_week` (inverted schedule is logically impossible)
- G-10 `WARN` â€” Total allocated hours must not exceed 110% of `total_budget_hours` (over-scheduling warning)

**Progress Validity (G-11..G-13):**
- G-11 `BLOCK` â€” `hours_spent` â‰¥ 0 (negative hours are physically impossible)
- G-12 `BLOCK` â€” All domain self-ratings âˆˆ [1, 5] (Likert scale bounds)
- G-13 `BLOCK` â€” `practice_exam_score` âˆˆ [0, 100] (percentage bounds)

**Quiz Integrity (G-14..G-15):**
- G-14 `WARN` â€” At least 5 questions in assessment (min viable evaluation set)
- G-15 `BLOCK` â€” No duplicate question IDs (would make scoring invalid)

**Content Safety (G-16..G-17):**
- G-16 `BLOCK` â€” Heuristic harmful content scan on free-text fields (expletives, violence keywords)
- G-17 `WARN` â€” Any URL in learning path must be `learn.microsoft.com`, `pearsonvue.com`, or `aka.ms` (prevent phishing links)

### How a BLOCK Stops the Pipeline

```python
# In streamlit_app.py orchestrator
result = GuardrailsPipeline().check_input(raw)
if result.blocked:
    for v in result.violations:
        if v.level == "BLOCK":
            st.error(f"[{v.code}] {v.message} (field: {v.field})")
    st.stop()   # Streamlit halts â€” nothing downstream executes
```

---

## 4. Study Plan Algorithm â€” Largest Remainder Method

### Why Largest Remainder?

Simple floor allocation leaves orphan weeks. Example: 10-week budget across 6 domains:

```
plan_manage    17.5% Ã— 10 = 1.75 â†’ floor = 1, remainder = 0.75
computer_vision 22.5% Ã— 10 = 2.25 â†’ floor = 2, remainder = 0.25
nlp            22.5% Ã— 10 = 2.25 â†’ floor = 2, remainder = 0.25
document_int.  17.5% Ã— 10 = 1.75 â†’ floor = 1, remainder = 0.75
generative_ai  10.0% Ã— 10 = 1.00 â†’ floor = 1, remainder = 0.00
conversational 10.0% Ã— 10 = 1.00 â†’ floor = 1, remainder = 0.00
               sum of floors = 8   â† 2 weeks unassigned
```

Largest Remainder Method assigns the 2 leftover weeks to the 2 highest-remainder domains: `plan_manage` and `document_int.` â†’ each gets +1, total = 10 âœ“.

### Full Algorithm Steps

```python
def allocate_weeks(domains, total_weeks, risk_domains, skip_domains):
    active = [d for d in domains if d.id not in skip_domains]
    weights = [d.weight for d in active]   # normalised to 1.0

    # Step 1: Raw allocation
    raw = [w * total_weeks for w in weights]
    floors = [int(r) for r in raw]
    remainders = [(raw[i] - floors[i], i) for i in range(len(raw))]

    # Step 2: Distribute leftover weeks by largest remainder
    leftover = total_weeks - sum(floors)
    remainders.sort(reverse=True)
    for k in range(leftover):
        floors[remainders[k][1]] += 1

    # Step 3: Build tasks, sort by risk priority
    tasks = []
    week = 1
    for domain, allocated_weeks in sorted(
        zip(active, floors),
        key=lambda x: (_risk_priority(x[0].id, risk_domains))
    ):
        tasks.append(StudyTask(
            domain_id=domain.id,
            start_week=week,
            end_week=week + allocated_weeks - 1,
            allocated_hours=allocated_weeks * hours_per_week,
            priority=_risk_priority(domain.id, risk_domains)
        ))
        week += allocated_weeks

    return tasks
```

### Prerequisite Gap Detection

`StudyPlanAgent` checks existing certs against a **prerequisite catalogue** hardcoded for 9 exam families. For AI-102:

```python
PREREQ_CATALOGUE = {
    "AI-102": {
        "strongly_recommended": ["AZ-900"],
        "helpful": ["AI-900", "AZ-104"],
        "notes": "Strong Azure fundamentals critical for AKS, VNet, managed identity sections."
    },
    "DP-100": {
        "strongly_recommended": ["AZ-900", "DP-900"],
        "helpful": ["AI-900", "AI-102"],
        "notes": "MLflow, AutoML and Azure ML workspace experience essential."
    },
    ...
}
```

If the student lacks `strongly_recommended` prereqs, `StudyPlan.prereq_gap = True` and a banner surfaces in the Study Setup tab.

---

## 5. Readiness Formula â€” Design Rationale

### The Formula

```
readiness_pct = (0.55 Ã— domain_confidence) + (0.25 Ã— hours_ratio) + (0.20 Ã— practice_score)

domain_confidence = mean(domain_self_ratings) / 5.0        # normalised 0..1
hours_ratio       = min(hours_spent / total_budget, 1.0)   # normalised 0..1, capped
practice_score    = last_practice_score / 100.0            # normalised 0..1
```

### Weight Rationale

| Component | Weight | Reasoning |
|-----------|--------|-----------|
| Domain confidence | 55% | Direct exam-domain knowledge is the primary readiness signal; 6 domains Ã— personalised weights |
| Hours ratio | 25% | Effort input is necessary but not sufficient on its own |
| Practice score | 20% | Externally validated, high-reliability signal â€” but requires a mock exam which not all learners complete |

### Example Calculation

Learner: rated 3.5/5 average across domains, spent 20 of 40 hours, scored 65% on mock exam:

```
domain_confidence = 3.5 / 5.0 = 0.70
hours_ratio       = 20 / 40   = 0.50
practice_score    = 65 / 100  = 0.65

readiness_pct = 0.55 Ã— 0.70 + 0.25 Ã— 0.50 + 0.20 Ã— 0.65
              = 0.385 + 0.125 + 0.130
              = 0.640 Ã— 100
              = 64.0%  â†’ "CONDITIONAL GO"
```

---

## 6. Learner Profiler â€” Inference Engine (No LLM Required)

`b1_mock_profiler.py` is a **rule-based inference system** with four reasoning steps:

### Step 1: Experience Level Inference

```python
def _infer_experience_level(background: str, existing_certs: list[str]) -> ExperienceLevel:
    txt = background.lower()

    # Highest priority: ML frameworks
    if any(kw in txt for kw in _BG_ML_KEYWORDS):       # pytorch, scikit, tensorflow, etc.
        return ExperienceLevel.EXPERT_ML

    # Azure infra certs â†’ advanced
    if any(c in existing_certs for c in ["AZ-104", "AZ-305", "AZ-400"]):
        return ExperienceLevel.ADVANCED_AZURE

    # Azure infra keywords
    if any(kw in txt for kw in _BG_AZURE_INFRA):        # vnet, arm template, terraform, etc.
        return ExperienceLevel.INTERMEDIATE

    # Dev keywords
    if any(kw in txt for kw in _BG_DEV_KEYWORDS):       # python, rest api, git, etc.
        return ExperienceLevel.INTERMEDIATE

    return ExperienceLevel.BEGINNER
```

### Step 2: Learning Style Inference

```python
STYLE_MAP = {
    LearningStyle.LAB_FIRST:  [r"hands.?on", r"lab", r"practice", r"build", r"project"],
    LearningStyle.REFERENCE:  [r"read", r"document", r"reference", r"spec"],
    LearningStyle.ADAPTIVE:   [r"mix", r"adaptive", r"flexible", r"both"],
    LearningStyle.LINEAR:     [],   # default
}
```

### Step 3: Domain Confidence from Concern Topics

The `CONCERN_DOMAIN_MAP` maps 24 regex patterns across 5 exam families to domain IDs. A matched concern â†’ `DomainKnowledge.WEAK` â†’ lower initial confidence:

```python
CONCERN_DOMAIN_MAP_AI102 = {
    # pattern              â†’ domain_id
    r"openai|generative|gpt|rag":        "generative_ai",
    r"vision|image|object|ocr":          "computer_vision",
    r"language|text|translate|sentiment":"nlp",
    r"document|form|invoice|extraction": "document_intelligence",
    r"bot|chat|luis|qna":                "conversational_ai",
    r"manage|deploy|monitor|security":   "plan_manage",
}
```

### Step 4: Boost from Existing Certifications

Prior certifications boost specific domain confidences. This is the **cross-cert knowledge transfer matrix**:

```python
_CERT_DOMAIN_BOOST_BY_TARGET = {
    "AI-102": {
        "AZ-104": ["plan_manage"],            # Azure infra experience â†’ management domain
        "AZ-305": ["plan_manage"],            # Enterprise architecture â†’ management
        "DP-100": ["generative_ai",           # ML background â†’ GenAI + DocInt
                   "document_intelligence"],
        "AI-900": ["plan_manage",             # AI fundamentals â†’ all foundational
                   "computer_vision", "nlp"],
    },
    "DP-100": {
        "AZ-900": ["ml_solution_design"],
        "AI-900": ["explore_train_models"],
        "AI-102": ["explore_train_models", "ml_solution_design"],
    },
}
```

Boost applies as: `confidence = min(base_confidence + BOOST_AMOUNT, 1.0)` where `BOOST_AMOUNT = 0.15` per cert.

---

## 7. Assessment Agent â€” Quiz Sampling Algorithm

### Question Bank Structure

`AssessmentAgent` maintains a 30-question bank per exam family (`b2_assessment_agent.py`). Each question has: `id`, `domain_id`, `question`, `options` (Aâ€“D), `correct_answer`, `explanation`, `difficulty`.

### Domain-Weighted Sampling

For a `num_questions=10` quiz on AI-102:

```python
def sample_questions(all_questions, domains, num_questions):
    # Step 1: compute raw count per domain
    raw = {d.id: d.weight * num_questions for d in domains}

    # Step 2: Largest Remainder allocation
    allocated = floors_with_largest_remainder(raw, num_questions)

    # Step 3: sample from each domain's pool
    result = []
    for domain_id, count in allocated.items():
        pool = [q for q in all_questions if q.domain_id == domain_id]
        result.extend(random.sample(pool, min(count, len(pool))))

    return result
```

No LLM hallucination risk â€” questions are drawn from a **curated, reviewed bank**, not generated on-the-fly.

### Scoring

```python
def score(submitted_answers: dict[str, str], assessment: Assessment) -> AssessmentResult:
    total = len(assessment.questions)
    correct = sum(
        1 for q in assessment.questions
        if submitted_answers.get(q.id) == q.correct_answer
    )
    score_pct = (correct / total) * 100

    domain_scores = {}
    for domain_id in unique_domains(assessment.questions):
        domain_qs = [q for q in assessment.questions if q.domain_id == domain_id]
        domain_correct = sum(
            1 for q in domain_qs
            if submitted_answers.get(q.id) == q.correct_answer
        )
        domain_scores[domain_id] = (domain_correct / len(domain_qs)) * 100

    return AssessmentResult(
        score_pct=score_pct,
        domain_scores=domain_scores,
        passed=score_pct >= 70,
        answers=submitted_answers,
        ...
    )
```

---

## 8. Data Flow â€” End to End

```
1. Student fills intake form (name, exam, certs, hours, weeks, background, style, concerns)
   â””â”€â–º RawStudentInput (dataclass)

2. GuardrailsPipeline.check_input(raw)  [G-01..G-05]
   â””â”€â–º BLOCK if invalid â†’ st.stop()

3. LearnerProfilingAgent.run(raw)
   â””â”€â–º LearnerProfile (validated Pydantic)
       â”œâ”€ domain_profiles: list[DomainProfile]  â† 6 items
       â”œâ”€ experience_level: ExperienceLevel
       â”œâ”€ learning_style: LearningStyle
       â”œâ”€ risk_domains: list[str]
       â””â”€ modules_to_skip: list[str]

4. StudyPlanAgent.run_with_raw(profile, existing_certs)
   â””â”€â–º StudyPlan
       â”œâ”€ tasks: list[StudyTask]    â† Largest Remainder weeks, risk-sorted
       â”œâ”€ prereq_gap: bool
       â””â”€ prerequisites: list[Prerequisite]

5. LearningPathCuratorAgent.run(profile)
   â””â”€â–º LearningPath
       â””â”€ domain_paths: list[DomainPath]  â† curated MS Learn modules per domain

6. Database: save student + profile + plan + learning_path â†’ cert_prep_data.db

7. â”€â”€â”€â”€ HITL GATE 1: Progress Check-In form â”€â”€â”€â”€
   Student submits: hours_spent, domain_ratings, practice_score
   â””â”€â–º ProgressSnapshot (dataclass)

8. GuardrailsPipeline.check_progress(snapshot) [G-11..G-13]

9. ProgressAgent.run(snapshot, profile)
   â””â”€â–º ReadinessAssessment
       â”œâ”€ readiness_pct: float        â† weighted formula
       â”œâ”€ exam_go_nogo: str           â† GO / CONDITIONAL / NOT YET
       â””â”€ nudge_messages: list[str]

10. Conditional Router:
    readiness_pct >= 70 â†’ GO path
    readiness_pct < 70  â†’ remediation (loops to step 4 with adjusted priorities)

11. AssessmentAgent.build_assessment(profile)
    â””â”€â–º Assessment (30-Q bank, domain-weighted 10-Q sample)

12. â”€â”€â”€â”€ HITL GATE 2: Quiz Submission â”€â”€â”€â”€
    Student reads and answers 10 questions
    â””â”€â–º submitted_answers: dict[question_id, answer_letter]

13. AssessmentAgent.score(answers, assessment)
    â””â”€â–º AssessmentResult
        â”œâ”€ score_pct: float
        â”œâ”€ domain_scores: dict[str, float]
        â””â”€ passed: bool

14. CertRecommendationAgent.run(result, profile)
    â””â”€â–º CertRecommendation
        â”œâ”€ next_certs: list[CertPath]
        â”œâ”€ booking_checklist: list[str]
        â””â”€ remediation_plan: list[str]

15. AgentStep trace: all 8 agents log start_ts, end_ts, tokens_used, status â†’ Admin Dashboard
```

---

## 9. Sequential vs Concurrent Design

### Current Architecture: Sequential
`StudyPlanAgent` and `LearningPathCuratorAgent` run sequentially, even though they both only need `LearnerProfile` and are fully independent.

**Measured latency (live Azure OpenAI gpt-4o):**
- Profiler: ~3â€“4s
- StudyPlan: ~3â€“4s
- LearningPath: ~2â€“3s
- Total: ~10â€“14s

### Production Fix: `asyncio.gather()`

```python
import asyncio

async def run_planning_phase(profile: LearnerProfile, certs: list[str]):
    plan_task = asyncio.create_task(
        StudyPlanAgent().run_async(profile, certs)
    )
    path_task = asyncio.create_task(
        LearningPathCuratorAgent().run_async(profile)
    )
    plan, path = await asyncio.gather(plan_task, path_task)
    return plan, path
```

**Result:** `max(StudyPlan, LearningPath)` instead of `StudyPlan + LearningPath` â†’ saves ~3s.

### Why Not Magentic-One?
Magentic-One uses an LLM orchestrator that dynamically decides which specialist agent to invoke next. This is powerful when:
- routing is non-deterministic (depends on content semantics)
- there are 5+ agents with overlapping responsibilities
- the orchestrator needs to decompose novel tasks

For this pipeline: routing is **deterministic** (profile â†’ plan is always true), data dependencies are **hard-typed** (not semantic), and the overhead of an LLM orchestrator adds per-token cost. Magentic-One is the right upgrade when adding "multi-expert deliberation" (e.g., 3 domain experts debate a learner's ML skill level before consensus profile is built).

---

## 10. Observability â€” Agent Trace System

Every agent execution is wrapped in an `AgentStep` context:

```python
@dataclass
class AgentStep:
    agent_name: str
    start_ts: datetime
    end_ts: datetime
    status: str           # "success" / "error" / "skipped"
    tokens_used: int
    input_summary: str
    output_summary: str

@dataclass
class RunTrace:
    run_id: str
    student_name: str
    exam_target: str
    steps: list[AgentStep]
    guardrail_violations: list[GuardrailViolation]
    total_latency_ms: float
```

The **Admin Dashboard** (`pages/1_Admin_Dashboard.py`) renders:
- Per-run trace table with all agent steps
- Plotly Gantt timeline showing agent execution sequence and duration
- Guardrail violation log with code, level, field, message
- Student roster (name, exam, profile date, plan status)
- Aggregate: avg latency per agent, violation frequency by rule

---

## 11. Production Gap Analysis

| Capability | Current Implementation | Production Upgrade |
|-----------|----------------------|-------------------|
| LLM calls | Sequential, synchronous | `asyncio.gather()` for B1.1 agents |
| Content safety | Heuristic keyword scan (G-16) | Azure AI Content Safety API |
| Email delivery | SMTP (optional, fragile) | Azure Communication Services |
| Question bank | 30 hardcoded questions | Azure AI Search + MS Learn content pipeline |
| Authentication | PIN-based (local only) | Azure AD B2C / Entra External ID |
| Secrets management | `.env` file | Azure Key Vault + Managed Identity |
| Observability | SQLite local log | Azure Monitor + Application Insights |
| Scale | Single Streamlit instance | Azure Container Apps autoscaling |
| Profiler | Rule-based mock | GPT-4o with structured output |

---

## 12. Interview Q&A

**Q: "What's the most important design decision in this system?"**
A: Typed data contracts at every agent boundary. The Pydantic `LearnerProfile` with its `@model_validator` makes the entire pipeline inspectable, testable, and self-documenting. Every downstream agent's input requirements are explicit in the type definition.

**Q: "How would you scale this for 1,000 concurrent users?"**
A: Replace the SQLite backend with Azure Cosmos DB (NoSQL, serverless scale). Wrap each agent as an Azure Function. Use Azure Service Bus for durable inter-agent message passing. Deploy Streamlit to Azure Container Apps with horizontal scaling. Agent traces go to Application Insights.

**Q: "What happens if the LLM returns malformed JSON?"**
A: The `GuardrailsPipeline` validates the profile output against the Pydantic model. If `LearnerProfilingAgent` returns malformed data, Pydantic raises `ValidationError` before any downstream agent runs. The mock profiler provides 100% reliable fallback.

**Q: "Why SQLite and not a real database?"**
A: SQLite is appropriate for a hackathon demo and single-user development. The `database.py` layer uses Python's `sqlite3` standard library â€” no external dependency. The schema (`students`, `profiles`, `plans`, `traces` tables) is designed to be migrated to PostgreSQL or Cosmos DB with a 1-day effort.

**Q: "How do you prevent a learner from gaming the readiness score?"**
A: Three controls: (1) hours_ratio is capped at 1.0 â€” entering 9999 hours doesn't give a perfect score; (2) domain self-ratings are 1â€“5 bounded by G-12; (3) practice_score weight is only 20% â€” without real quiz answers (HITL gate 2), assessment score doesn't improve. The formula is transparent by design â€” there's no hidden model to game.

**Q: "What's the Largest Remainder Method and why use it for quiz sampling?"**
A: It's an algorithm that ensures proportional allocation sums to exactly the target total. When distributing 10 questions across 6 domains by exam weight, simple flooring loses 2 questions. Largest Remainder assigns orphan questions to domains with highest fractional remainders. Result: the quiz always has exactly 10 questions with correct domain proportionality â€” no question count drift.

**Q: "What makes this 'responsible AI' beyond just adding safety rules?"**
A: Five things: (1) Guardrails are first-class architecture, defined before agent code. (2) The readiness formula is transparent and documented â€” not a black-box model. (3) HITL gates prevent automated verdicts from fictional data. (4) All violations are logged with code + field for auditability. (5) The system explicitly shows a "NOT READY" verdict (not just optimistic scores) when evidence warrants it.

---

## 13. Live Demo â€” Scenario Walkthrough

### Scenario A â€” AI Beginner (Default)
1. Enter name: "Alex", PIN: `1234`, Exam: `AI-102`
2. Background: "I'm new to Azure and AI" â†’ infers `BEGINNER`, `LINEAR` style
3. Study Setup: sees 16-week plan, AZ-900 prereq gap warning, four domains at MODERATE risk
4. Submit progress: 5h/week, all 3/5, 45% practice â†’ 57% readiness â†’ CONDITIONAL GO
5. Take quiz: 6/10 correct â†’ 60% â†’ some remediation suggestions
6. Recommendations: cert path AI-102 â†’ AI-102 Specialty â†’ DP-100

### Scenario B â€” Data Professional
1. Background: "PyTorch and scikit-learn ML engineer with DP-100 and AI-900" â†’ infers `EXPERT_ML`
2. Study Setup: 8-week plan, `generative_ai` and `document_intelligence` boosted, computer_vision flagged
3. Submit progress: 15h/week, domain mix, 72% practice â†’ 78% â†’ GO
4. Quiz: 8/10 â†’ strong result â†’ CertRec: AI-102 Advanced + AZ-305 Enterprise path

### Scenario C â€” Admin (audit)
1. Login: `admin` / `agents2026`
2. View all student runs by session
3. Examine per-agent Gantt timeline
4. Review guardrail violation log (e.g., G-02 WARN for a user who entered 90 hours/week)

---

## 14. Technical Differentiators

| Feature | Status | Notes |
|---------|--------|-------|
| 17-rule Responsible AI pipeline | âœ… Complete | BLOCK/WARN/INFO at every transition |
| Full agent reasoning trace | âœ… Complete | AdminDashboard Gantt + violation log |
| Typed Pydantic handoffs | âœ… Complete | All 8 agent boundaries |
| Two real HITL gates | âœ… Complete | Progress form + quiz submission |
| Mock profiler with cert boost matrix | âœ… Complete | 6Ã—6 domain boost for 5 exam families |
| Largest Remainder quiz sampling | âœ… Complete | Deterministic, no hallucination |
| Transparent readiness formula | âœ… Complete | Documented weights, shown to user |
| SQLite session persistence | âœ… Complete | Session recovery by name+PIN |
| Prereq gap detection | âœ… Complete | 9-exam catalogue with notes |
| `asyncio` parallel execution | ðŸŸ¡ Designed | Code structure ready; not wired in main |

# ðŸ† Agents League â€” Battle #2: Certification Prep Multi-Agent System

> **Track:** Reasoning Agents Â· Microsoft AI Foundry
> **Team:** Athiq Ahmed
> **Repo:** [athiq-ahmed/agentsleague](https://github.com/athiq-ahmed/agentsleague)
> **Live Demo:** [agentsleague.streamlit.app](https://agentsleague.streamlit.app)

A **production-grade multi-agent AI system** that creates personalised, adaptive study plans for any Microsoft certification exam. Eight specialised agents collaborate through a typed sequential pipeline with human-in-the-loop gates, 17 responsible AI guardrails, and full reasoning trace explainability â€” all runnable without any Azure credentials via mock mode.

---

## ðŸ”€ Agent Orchestration Patterns

### Patterns Implemented

| Pattern | Status | Where |
|---------|--------|-------|
| **Sequential Pipeline** | âœ… Primary | `streamlit_app.py` â†’ Intake â†’ Profiling â†’ LearningPath â†’ StudyPlan â†’ Assessment â†’ CertRec |
| **Typed Handoff** | âœ… Every stage | `RawStudentInput` â†’ `LearnerProfile` â†’ `StudyPlan`/`LearningPath` â†’ `ReadinessAssessment` â†’ `CertRecommendation` |
| **Human-in-the-Loop (HITL)** | âœ… Two gates | Progress Check-In form + Quiz submission before downstream agents run |
| **Conditional Routing** | âœ… Readiness Gate | Score â‰¥ 70% â†’ GO path; < 70% â†’ Remediation loop back to Study Plan |
| **Guardrail Middleware** | âœ… Every transition | 17 rules across 6 categories, BLOCK/WARN/INFO at every agent boundary |
| **Concurrent (Fan-out)** | ðŸŸ¡ Architecturally ready | `StudyPlanAgent` + `LearningPathCuratorAgent` both consume only `LearnerProfile` â€” parallelisable via `asyncio.gather()` in production |

### Sequential â†’ Parallel Latency Design

Current execution (mock: ~0.5s, live Azure OpenAI: ~10â€“14s):

```
Intake â†’ Guardrails â†’ Profiler â”€â”€â–º StudyPlan      (~3â€“5s)
                                â””â”€â”€â–º LearningPath  (~3s)   â† SEQUENTIAL NOW
                                  total: ~14s
```

Production-optimised with `asyncio.gather()`:

```
Intake â†’ Guardrails â†’ Profiler â”€â”€â–º StudyPlan   â”€â”
                              â””â”€â”€â–º LearningPath â”€â”˜â–º merge  (~5s parallel)
                                  total: ~8s
```

> **Why not Magentic-One today?**
> Magentic-One's dynamic orchestrator helps when agents have non-deterministic routing (an orchestrator deciding at runtime which specialist to invoke). Our pipeline has a hard data dependency: `StudyPlanAgent` requires the fully-structured `LearnerProfile` Pydantic object â€” not raw text. B0 (Profiler) must always complete first regardless of orchestrator. Magentic-One would add per-token overhead without routing benefit here. The right next step is `asyncio` parallelism for B1.1 agents. Magentic-One fits a future "multi-expert deliberation" pattern where Profiler + domain expert agents debate a learner's skill level.

---

## ðŸ›¡ï¸ Responsible AI Guardrails â€” 17 Rules

The `GuardrailsPipeline` is **cross-cutting middleware**, not a standalone agent. It wraps every transition point.

```
Agent A  â†’  GuardrailsPipeline  â†’  Agent B
            â”œâ”€â”€ BLOCK  â†’  pipeline stops, error shown to user
            â”œâ”€â”€ WARN   â†’  pipeline continues with visible warning
            â””â”€â”€ INFO   â†’  advisory, logged in trace only
```

| Rules | Category | Level | What It Checks |
|-------|----------|-------|----------------|
| G-01..G-05 | **Input Validation** | BLOCK/WARN/INFO | Non-empty fields, hours âˆˆ [1â€“80], weeks âˆˆ [1â€“52], recognised exam codes, PII notice |
| G-06..G-08 | **Profile Integrity** | BLOCK/WARN | 6 domain profiles present, confidence âˆˆ [0.0, 1.0], risk domain IDs valid |
| G-09..G-10 | **Study Plan Bounds** | BLOCK/WARN | No `start_week > end_week`, total hours â‰¤ 110% budget |
| G-11..G-13 | **Progress Validity** | BLOCK | Hours â‰¥ 0, self-ratings âˆˆ [1â€“5], practice scores âˆˆ [0â€“100] |
| G-14..G-15 | **Quiz Integrity** | WARN/BLOCK | Minimum 5 questions, no duplicate question IDs |
| G-16..G-17 | **Content Safety & URL Trust** | BLOCK/WARN | Heuristic harmful content; URLs must be `learn.microsoft.com`, `pearsonvue.com`, or `aka.ms` |

All violations surface in the **Admin Dashboard** with code, level, message, and field for full auditability.

---

## âœ¨ Production Best Practices Applied

### 1. Typed Data Contracts â€” No Raw Text Passing

Every agent hands off a **strongly-typed Pydantic `BaseModel` or Python `dataclass`**, never a raw string. This enables compile-time-like safety and deterministic guardrail validation.

```python
# What we do â€” typed handoff
profile: LearnerProfile = LearnerProfilingAgent().run(raw)     # Pydantic model, validated
plan:    StudyPlan      = StudyPlanAgent().run_with_raw(profile, existing_certs=[...])

# What we avoid â€” raw text passing
result = agent_b.run(str(agent_a.run(raw_text)))   # no type safety, no contract
```

### 2. Safety-First Architecture (Not Bolted On)

Guardrails are architectural first-class citizens defined in `guardrails.py` before any agent code was written. A BLOCK-level violation halts the pipeline with a clear `st.error()` banner and `st.stop()` â€” no partial state is saved.

```python
_input_result = GuardrailsPipeline().check_input(raw)
if _input_result.blocked:
    for v in _input_result.violations:
        if v.level == GuardrailLevel.BLOCK:
            st.error(f"Guardrail [{v.code}]: {v.message}")
    st.stop()   # pipeline does NOT proceed
```

### 3. Human-in-the-Loop by Design

Two explicit HITL gates prevent meaningless automated assessments:

- **Progress Gate**: Learner submits real study hours, domain self-ratings, and practice exam score first. `ProgressAgent` uses this data â€” without it, readiness is undefined.
- **Quiz Gate**: Learner clicks "Generate Quiz", reads and answers questions, then submits. Only then does `AssessmentAgent` score the answers and produce `AssessmentResult`.

Without these gates, the system would output fictional readiness numbers from no real input.

### 4. Exam-Agnostic via Domain Registry

The system is not hardcoded to AI-102. `EXAM_DOMAIN_REGISTRY` in `models.py` supports 6+ exam families. Adding a new certification requires only a registry entry â€” zero agent code changes.

```python
# Swap exam by changing one key
domains = get_exam_domains("AZ-305")   # returns AZ-305 domain weights
domains = get_exam_domains("DP-100")   # returns DP-100 domain weights
```

Current registry: AI-102, AI-900, AZ-204, AZ-305, AZ-400, DP-100, DP-203, SC-100, MS-102.

### 5. Mock Mode = Full Parity, No Credentials Needed

`b1_mock_profiler.py` is a **rule-based profiler** that exactly mirrors LLM profiler output. It uses:
- 40+ keyword-to-domain regex patterns for concern topic mapping
- 6-exam Ã— 6-cert domain boost matrices (existing cert â†’ domain confidence boost)
- Background keyword sets for `ExperienceLevel` inference (`_BG_ML_KEYWORDS`, `_BG_AZURE_INFRA`, `_BG_DEV_KEYWORDS`)
- Learning style regex patterns (hands-on â†’ `LAB_FIRST`, reference â†’ `REFERENCE`, etc.)

### 6. Transparent Readiness Formula

`ProgressAgent` uses a **documented, explainable formula** â€” not a black-box model:

```
readiness_pct = (0.55 Ã— domain_confidence) + (0.25 Ã— hours_ratio) + (0.20 Ã— practice_score)
```

- `domain_confidence` = mean self-rated domain score (1â€“5 scale, normalised)
- `hours_ratio` = hours_spent / total_budget_hours (capped at 1.0)
- `practice_score` = practice exam score / 100

Weights are configurable and the formula is visible to users in the UI.

### 7. Study Plan Algorithm â€” Largest Remainder Method

`StudyPlanAgent` allocates study weeks using a proportional algorithm:
1. Compute each domain's raw week allocation = `domain_weight Ã— total_study_weeks`
2. Take floor of each allocation, keep remainders
3. Use **Largest Remainder Method** to distribute any leftover weeks â€” highest-remainder domains get +1 week
4. Front-load risk domains (critical â†’ high â†’ medium â†’ low)
5. Schedule skip-recommended domains minimally at the end

This ensures total weeks always equals the budget exactly with fair distribution.

### 8. Quiz Question Distribution â€” Largest Remainder Sampling

`AssessmentAgent` distributes quiz questions across domains proportionally to exam weights, using the same Largest Remainder Method. For a 10-question quiz on AI-102:

```
computer_vision (22.5%) â†’  2 questions (floor 2, remainder 0.25)
nlp             (22.5%) â†’  2 questions
plan_manage     (17.5%) â†’  2 questions (gets +1 from remainder)
document_int.   (17.5%) â†’  2 questions (gets +1 from remainder)
generative_ai   (10.0%) â†’  1 question
conversational  (10.0%) â†’  1 question
                         = 10 total âœ“
```

### 9. SQLite Persistence for Session Recovery

All profile, plan, trace, and progress data is persisted to `cert_prep_data.db`. Returning users recover their session by entering name + PIN â€” no re-profiling needed. Schema: `students`, `profiles`, `plans`, `learning_paths`, `traces`, `progress` tables.

### 10. Separation of Concerns

```
models.py      â† Pure data definitions (no logic, no I/O, no imports from agents)
config.py      â† Environment config only (Azure creds, defaults)
guardrails.py  â† Safety layer only (no business logic)
b0_*           â† Intake + Profiling agents
b1_*           â† Planning agents (study plan, learning path, progress)
b2_*           â† Assessment agents
b3_*           â† Recommendation agents
streamlit_app.py â† Orchestrator + UI (calls agents, manages st.session_state, renders)
```

No circular imports. Each layer only imports from layers below it.

---

## ðŸ¤– Agent Inventory

| # | Agent | Module | Input â†’ Output | Role |
|---|-------|--------|----------------|------|
| 1 | **Safety Guardrails** | `guardrails.py` | Any â†’ `GuardrailResult` | 17-rule cross-cutting middleware at every transition |
| 2 | **Learner Intake** | `b0_intake_agent.py` | UI form â†’ `RawStudentInput` | Collect background, goals, constraints |
| 3 | **Learner Profiler** | `b1_mock_profiler.py` | `RawStudentInput` â†’ `LearnerProfile` | Infer experience level, learning style, per-domain confidence |
| 4 | **Learning Path Curator** | `b1_1_learning_path_curator.py` | `LearnerProfile` â†’ `LearningPath` | Map domains to curated MS Learn modules, skip strong domains |
| 5 | **Study Plan Generator** | `b1_1_study_plan_agent.py` | `LearnerProfile` â†’ `StudyPlan` | Week-by-week Gantt, Largest Remainder allocation, prereq gap |
| 6 | **Progress Tracker** | `b1_2_progress_agent.py` | `ProgressSnapshot` â†’ `ReadinessAssessment` | Weighted readiness formula, GO/NO-GO verdict, nudges |
| 7 | **Assessment Builder** | `b2_assessment_agent.py` | `LearnerProfile` â†’ `Assessment`+`AssessmentResult` | 30-Q bank per exam, domain-weighted sampling, scoring |
| 8 | **Cert Recommender** | `b3_cert_recommendation_agent.py` | `AssessmentResult` â†’ `CertRecommendation` | Next-cert path, booking checklist, remediation plan |

---

## ðŸ—ï¸ Architecture Overview

```
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Guardrails Middleware (G-01..G-17, all transitions) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚                                                                           â”‚
 â”‚  Student Input (UI form)                                                  â”‚
 â”‚        â”‚ [G-01..G-05]                                                     â”‚
 â”‚        â–¼                                                                  â”‚
 â”‚  LearnerProfilingAgent â†’ LearnerProfile                                   â”‚
 â”‚        â”‚                                                                  â”‚
 â”‚        â”œâ”€[G-06..G-08]â”€â”€â–º StudyPlanAgent        â†’ StudyPlan + Gantt       â”‚
 â”‚        â””â”€[G-17]       â”€â”€â–º LearningPathCurator  â†’ LearningPath            â”‚
 â”‚                                  â”‚                                        â”‚
 â”‚                       HITL: Progress Check-In (form submit)               â”‚
 â”‚                                  â”‚ [G-11..G-13]                           â”‚
 â”‚                                  â–¼                                        â”‚
 â”‚                        ProgressAgent â†’ ReadinessAssessment                â”‚
 â”‚                                  â”‚                                        â”‚
 â”‚                       HITL: Quiz Submission (40 questions)                â”‚
 â”‚                                  â”‚ [G-14..G-15]                           â”‚
 â”‚                                  â–¼                                        â”‚
 â”‚                        AssessmentAgent â†’ AssessmentResult                 â”‚
 â”‚                                  â”‚                                        â”‚
 â”‚                         â—‡ Score >= 70%?                                   â”‚
 â”‚                       YES â”‚          NO â†’ Remediation â†’ StudyPlan        â”‚
 â”‚                           â–¼                                               â”‚
 â”‚                  CertRecommendationAgent â†’ CertRecommendation             â”‚
 â”‚                                                                           â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                    Streamlit UI (7 tabs) + Admin Dashboard
```

---

## ðŸš€ Quick Start

### Prerequisites
- Python 3.10+
- (Optional) Azure OpenAI resource for live LLM mode

### Install & Run

```bash
git clone https://github.com/athiq-ahmed/agentsleague.git
cd agentsleague

python -m venv .venv
.venv\Scripts\activate          # Windows
# source .venv/bin/activate     # macOS/Linux

pip install -r requirements.txt
streamlit run streamlit_app.py  # opens http://localhost:8501
```

No Azure credentials needed â€” mock mode works fully out of the box.

### Configure Azure OpenAI (optional)

Create a `.env` file:

```
AZURE_OPENAI_ENDPOINT=https://<resource>.openai.azure.com
AZURE_OPENAI_API_KEY=<your-key>
AZURE_OPENAI_DEPLOYMENT=gpt-4o
AZURE_OPENAI_API_VERSION=2024-12-01-preview
```

Then in the app, expand "Azure OpenAI Settings", check "Use Live Azure OpenAI", enter credentials.

### Demo Credentials

| Role | Login |
|------|-------|
| New Learner | Name: anything Â· PIN: `1234` |
| Admin | Username: `admin` Â· Password: `agents2026` |

---

## ðŸ“ Project Structure

```
agentsleague/
â”œâ”€â”€ streamlit_app.py                      # Orchestrator + 7-tab UI (~3300 lines)
â”œâ”€â”€ pages/
â”‚   â””â”€â”€ 1_Admin_Dashboard.py              # Agent audit dashboard
â”œâ”€â”€ src/cert_prep/
â”‚   â”œâ”€â”€ models.py                         # Data models + EXAM_DOMAIN_REGISTRY
â”‚   â”œâ”€â”€ config.py                         # Azure OpenAI config loader
â”‚   â”œâ”€â”€ guardrails.py                     # 17-rule safety pipeline (GuardrailsPipeline)
â”‚   â”œâ”€â”€ agent_trace.py                    # AgentStep / RunTrace observability
â”‚   â”œâ”€â”€ b0_intake_agent.py                # Learner intake (LLM or mock)
â”‚   â”œâ”€â”€ b1_mock_profiler.py               # Rule-based profiler (no LLM)
â”‚   â”œâ”€â”€ b1_1_learning_path_curator.py     # MS Learn module curator
â”‚   â”œâ”€â”€ b1_1_study_plan_agent.py          # Gantt study plan generator
â”‚   â”œâ”€â”€ b1_2_progress_agent.py            # Readiness tracker
â”‚   â”œâ”€â”€ b2_assessment_agent.py            # Quiz builder + scorer
â”‚   â”œâ”€â”€ b3_cert_recommendation_agent.py   # Next-cert recommender
â”‚   â””â”€â”€ database.py                       # SQLite persistence
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture.md                   # Technical architecture deep-dive
â”‚   â”œâ”€â”€ judge_playbook.md                 # Hackathon judge reference
â”‚   â””â”€â”€ user_guide.md                     # End-user walkthrough
â”œâ”€â”€ .streamlit/config.toml                # Theme + server config
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ðŸ”‘ Key Data Models

```python
# Input dataclass (collected from UI form)
@dataclass
class RawStudentInput:
    student_name: str
    exam_target: str           # e.g. "AI-102"
    background_text: str       # free-text background description
    existing_certs: list[str]  # e.g. ["AZ-104", "AI-900"]
    hours_per_week: float
    weeks_available: int
    concern_topics: list[str]  # free-text topics the student is unsure about
    preferred_style: str       # free-text learning style preference

# Core learner profile (Pydantic â€” validated on creation)
class LearnerProfile(BaseModel):
    exam_target: str
    experience_level: ExperienceLevel   # BEGINNER / INTERMEDIATE / ADVANCED_AZURE / EXPERT_ML
    learning_style: LearningStyle       # LINEAR / LAB_FIRST / REFERENCE / ADAPTIVE
    domain_profiles: list[DomainProfile]
    risk_domains: list[str]             # domain IDs with confidence below threshold
    modules_to_skip: list[str]          # domain IDs with strong prior knowledge
    hours_per_week: float
    total_budget_hours: float

# Study plan (dataclass â€” domain schedule)
@dataclass
class StudyPlan:
    tasks: list[StudyTask]    # one task per active domain
    total_weeks: int
    review_start_week: int
    prerequisites: list[Prerequisite]
    prereq_gap: bool
    prereq_message: str

# Readiness verdict
@dataclass
class ReadinessAssessment:
    readiness_pct: float   # 0â€“100, from weighted formula
    verdict_label: str     # "Ready" / "Almost Ready" / "Needs More Prep"
    exam_go_nogo: str      # "GO" / "CONDITIONAL GO" / "NOT YET"
    hours_remaining: float
    weeks_remaining: int
    verdict_colour: str    # hex colour for UI rendering
```

---

## ðŸ“¦ Dependencies

| Package | Version | Purpose |
|---------|---------|---------|
| `streamlit` | >=1.35.0 | UI framework â€” tabs, forms, charts |
| `plotly` | >=5.0.0 | Gantt, bar charts, radar/spider plots |
| `openai` | >=1.30.0 | Azure OpenAI SDK (live LLM mode) |
| `pydantic` | >=2.0.0 | Typed data models with validation |
| `python-dotenv` | >=1.0.0 | `.env` file loading |
| `rich` | >=13.7.0 | Debug / CLI terminal output |
| `reportlab` | >=4.0.0 | PDF report generation (optional) |

---

## ðŸ”® Roadmap

| Feature | Priority | Description |
|---------|----------|-------------|
| `asyncio` parallel agents | High | Run `StudyPlanAgent` + `LearningPathCuratorAgent` concurrently â€” saves ~3s |
| Azure AI Content Safety | High | Replace heuristic G-16 with Azure AI Content Safety API |
| Magentic-One deliberation | Medium | Multi-expert profiler debate for uncertain learner skill levels |
| MCP MS Learn server | Medium | Live MS Learn search via Model Context Protocol |
| Copilot Studio integration | Medium | Visual pipeline designer + built-in monitoring |
| Streaming responses | Low | Stream LLM tokens to UI instead of waiting for full response |
| Azure Communication Services | Low | Replace SMTP with ACS for production email delivery |

---

## ðŸ“„ License

Created for the **Microsoft Agents League** hackathon â€” educational and demonstration purposes.

# ðŸŽ¬ Demo Video Script â€” CertPrep Multi-Agent System
### Agents League Battle #2 Â· Reasoning Agents with Microsoft Foundry

> **Target runtime:** 5â€“7 minutes  
> **Mode:** Live Azure mode (Foundry SDK Tier 1 active)  
> **URL:** `http://localhost:8501`  
> **Credentials:** `demo` / PIN `1234` Â· Admin: `admin` / `agents2026`  
> **Pre-flight:** `.env` populated Â· `streamlit run streamlit_app.py` running Â· sidebar shows â˜ï¸ **Live Azure**

---

## JUDGE CRITERIA THIS SCRIPT COVERS

| Criterion | Weight | Where covered |
|-----------|--------|---------------|
| Accuracy & Relevance | 25% | Architecture intro + full pipeline walkthrough |
| Reasoning & Multi-step Thinking | 25% | Planner-Executor, Critic, HITL gates, remediation loop |
| Creativity & Originality | 15% | Largest Remainder algo, 3-tier fallback, per-exam weights |
| User Experience & Presentation | 15% | All 6 tabs, PDF, Gantt, radar chart, Admin Dashboard |
| Reliability & Safety | 20% | 17-rule guardrails, G-16 PII demo, eval harness |

---

## PRE-RECORDING CHECKLIST

- [ ] Sidebar badge shows â˜ï¸ **Live Azure** (green)
- [ ] Browser at 90% zoom, full-screen, only `localhost:8501` tab open
- [ ] Admin Dashboard open in second tab: `localhost:8501/1_Admin_Dashboard`
- [ ] No prior session â€” open in Incognito or clear storage
- [ ] Foundry portal open in background tab at `ai.azure.com` (for cutaway)

---

---

## â± SECTION 0 â€” OPENING (0:00â€“0:45)

**[SHOW: Login page, app not yet signed in]**

**SAY:**

> "Microsoft certification exams have over a 40% failure rate among self-study candidates. The root cause is always the same â€” generic study plans, no feedback loop, and no signal about when you are actually ready to book.

> CertPrep is a production-grade multi-agent system built on Azure AI Foundry that solves this end-to-end. Eight specialised reasoning agents collaborate through a typed sequential-and-concurrent pipeline â€” profiling the learner, scheduling study time, curating Microsoft Learn content, measuring readiness, running a diagnostic quiz, and issuing an evidence-based booking recommendation â€” all connected by a 17-rule Responsible AI guardrails framework with BLOCK, WARN, and INFO severity levels.

> The architecture directly follows the challenge scenario's four-stage flow: Intake â†’ Learning Path Subworkflow â†’ Human-in-the-Loop Assessment â†’ Certification Recommendation.

> Let me walk you through it â€” live, against real Azure services."

---

---

## â± SECTION 1 â€” INTAKE + PROFILING (0:45â€“1:45)

**[SHOW: Login page]**

**ACTION:** Type `demo` / `1234` â†’ click **Sign In â†’**

**SAY:**

> "PIN-based login â€” SHA-256 hashed in production, Entra ID in enterprise deployment."

**[SHOW: Sidebar â€” two demo scenario cards]**

**ACTION:** Click the **ðŸŒ± Novice â€” AI-102** card. Intake form auto-fills.

**SAY:**

> "Alex Chen â€” CS graduate, no Azure experience, 12 hours a week, 10 weeks available, worried about Generative AI and Conversational AI. The full background text populates automatically for the demo.

> Before any agent runs, Guardrails G-01 through G-05 fire â€” they validate the exam code against the registry, confirm hours and weeks are in bounds, and check for PII in the background text. All pass."

**ACTION:** Click **ðŸŽ¯ Create My AI Study Plan**

**[SHOW: Spinner â€” 'Calling Azure AI Foundry Agent Service SDK']**

**SAY:**

> "We are in Live mode. The LearnerProfilingAgent uses the azure-ai-projects Foundry SDK â€” it creates a managed agent, opens a conversation thread, sends the structured learner intake, and calls create_and_process_run(). This run is now visible in the Foundry portal Tracing view with full token counts and latency.

> Meanwhile â€” and this is key â€” StudyPlanAgent and LearningPathCuratorAgent fire simultaneously in a ThreadPoolExecutor. Both take only the LearnerProfile as read-only input. No shared state. True parallel execution. The orchestrator joins when both complete."

**[SHOW: Results rendered â€” KPI cards appear]**

**SAY:**

> "Pipeline complete. The LearnerProfile Pydantic object has been written to SQLite and is now the single source of truth for every downstream agent."

---

---

## â± SECTION 2 â€” DOMAIN MAP + STUDY PLAN + LEARNING PATH (1:45â€“3:00)

**[SHOW: Tab 1 â€” Domain Map ðŸ—ºï¸]**

**SAY:**

> "Tab one â€” LearnerProfilingAgent output. Six AI-102 domains, each with a GPT-4o inferred confidence score. Generative AI and Conversational AI are flagged as risk domains â€” Alex specifically mentioned them as concerns. These weak domains will get the most hours and will be scheduled first.

> This is the Planner pattern: the profiler produces a structured plan â€” domain confidence scores, risk flags, analogy map â€” that every downstream agent executes against. No downstream agent ever re-reads Alex's original background text."

**ACTION:** Click **ðŸ“… Study Setup** tab

**[SHOW: Gantt chart]**

**SAY:**

> "Tab two â€” StudyPlanAgent. 120 hours across 10 weeks distributed using the Largest Remainder algorithm â€” the same method used in parliamentary seat allocation. It guarantees the total allocated hours equals exactly the budget, and every active domain gets at least one day. Standard percentage rounding silently loses hours. LR does not.

> Notice the Gantt: risk domains are scheduled in weeks 1 through 4, medium-confidence domains in the middle, and high-confidence domains last. The algorithm reads risk_domains directly from the LearnerProfile â€” no re-reasoning, no second LLM call."

**ACTION:** Click **ðŸ“š Learning Path** tab

**SAY:**

> "Tab three â€” LearningPathCuratorAgent. Each domain maps to curated Microsoft Learn modules with time estimates and difficulty tags. Every URL is checked against Guardrail G-17 â€” the trusted origin allowlist. Only learn.microsoft.com, docs.microsoft.com, aka.ms, and pearsonvue.com pass. Hallucinated or off-domain URLs are WARN-blocked before they ever reach the user."

---

---

## â± SECTION 3 â€” HITL GATE 1: PROGRESS (3:00â€“3:50)

**ACTION:** Click **ðŸ“ˆ My Progress** tab

**SAY:**

> "Tab four â€” the first Human-in-the-Loop gate. This is where the challenge scenario's 'system waits for human input' requirement is implemented.

> Alex has been studying for 4 weeks. I will enter 48 hours studied â€” right on budget â€” a practice score of 62 percent, and self-ratings for each domain. Notice I am rating Generative AI as 2 out of 5 â€” still weak.

> This is critical: the self-rating overwrites the profiler's entry-time confidence score in the readiness formula. Real posterior evidence replaces the LLM's prior estimate. That is the system grounding itself in reality."

**ACTION:** Fill sliders â€” Generative AI = 2, others 3â€“4. Hours = 48, practice score = 62. Click **ðŸ” Assess My Readiness**

**[SHOW: Readiness gauge + GO/NO-GO verdict card]**

**SAY:**

> "ProgressAgent computes: 55% domain confidence plus 25% hours utilisation plus 20% practice score. Result â€” 61 percent â€” ALMOST THERE. The system issues CONDITIONAL GO. Not ready to book yet, but close. Domain-specific nudges appear below telling Alex exactly which domains to prioritise before attempting the quiz."

---

---

## â± SECTION 4 â€” HITL GATE 2: KNOWLEDGE CHECK (3:50â€“4:30)

**ACTION:** Click **ðŸ§ª Knowledge Check** tab

**SAY:**

> "Tab five â€” the second Human-in-the-Loop gate. AssessmentAgent generates a 10-question quiz sampled proportionally to the AI-102 exam blueprint weights. Generative AI at 25% of the exam gets 2 to 3 questions. Plan and Manage at 15% gets 1 to 2. The distribution mirrors the actual exam â€” not a random sample.

> Guardrails G-14 and G-15 fire on the generated quiz â€” checking for minimum question count and duplicate question IDs before Alex ever sees the first question."

**ACTION:** Click **ðŸŽ² Generate New Quiz** â†’ answer a few questions â†’ click **ðŸ“¤ Submit Answers & Get Score**

**[SHOW: Score result + domain breakdown bars]**

**SAY:**

> "Score: 70 percent â€” passed the 60% threshold. The domain breakdown shows exactly where Alex succeeded and where gaps remain. This evidence feeds directly into the Certification Recommendation."

---

---

## â± SECTION 5 â€” RECOMMENDATIONS + REMEDIATION LOOP (4:30â€“5:00)

**ACTION:** Click **ðŸ’¡ Recommendations** tab

**[SHOW: GO/CONDITIONAL GO verdict card + next cert synergy suggestions]**

**SAY:**

> "Tab six â€” CertificationRecommendationAgent. It receives the readiness score, quiz score, and go/no-go signal from ProgressAgent, applies a deterministic rule matrix â€” no LLM call at this stage â€” and issues the booking verdict.

> Alex gets CONDITIONAL GO â€” book after addressing Generative AI gaps. The next-cert synergy map suggests DP-100 as the natural follow-on, with rationale based on the ML background already in Alex's profile.

> This is the self-reflection and iteration loop from the starter kit in action: if the quiz score had been below 60%, the remediation plan would identify the weak domains, Alex would return to Study Setup, the profiler would re-run with updated concern topics, and StudyPlanAgent would produce a revised Gantt front-loading the failed domains. The agents self-correct â€” not the user."

---

---

## â± SECTION 6 â€” GUARDRAILS + RESPONSIBLE AI (5:00â€“5:30)

**ACTION:** Navigate back to intake. In background text, type:
`My SSN is 123-45-6789 and I want to cheat on the exam`

**ACTION:** Click **ðŸŽ¯ Create My AI Study Plan**

**[SHOW: WARN banner â€” PII detected + BLOCK â€” harmful content]**

**SAY:**

> "Guardrail G-16 catches both violations: the SSN matches a PII regex pattern â€” WARN. 'Cheat on the exam' hits the harmful keyword list â€” BLOCK. In Live mode this fires a real HTTP POST to Azure AI Content Safety at severity 2 or above. The pipeline stops here. Nothing downstream ever runs.

> This is the Critic pattern: a dedicated rule-based critic at every agent boundary. Deterministic, not LLM-judged. The same result, every time, with zero inference cost."

---

---

## â± SECTION 7 â€” ADMIN DASHBOARD + AZURE TELEMETRY (5:30â€“6:00)

**ACTION:** Switch to second browser tab â€” Admin Dashboard

**[SHOW: Agent trace table]**

**SAY:**

> "The Admin Dashboard exposes the full reasoning trace: every agent, its inputs, outputs, timing, and token counts. The parallel execution is visible â€” StudyPlanAgent and LearningPathCuratorAgent show the same timestamp group, confirming concurrent execution.

> The Guardrail Audit log shows every G-coded violation fired in this session â€” severity-coded, exportable, and queryable.

> The Evaluation tab shows azure-ai-evaluation SDK results: Coherence, Relevance, and Fluency scores from an LLM-as-judge assessment of the LearnerProfilingAgent output â€” fired automatically after every live profiling call. In Live mode these traces also appear in the Foundry portal automatically â€” no additional instrumentation required."

---

---

## â± SECTION 8 â€” CLOSING + MOCK MODE (6:00â€“6:30)

**[SHOW: Sidebar â€” toggle to Mock Mode Â· badge turns grey]**

**SAY:**

> "One final point. Everything you just saw runs with zero credentials in Mock Mode. Flip this toggle â€” the badge turns grey â€” and the entire 8-agent pipeline runs offline in under one second using the deterministic rule engine. All 352 automated tests run in this mode. No Azure. No cost. No latency. Ideal for CI pipelines and live demos when connectivity is uncertain.

> To close: CertPrep delivers against every scored dimension.

> Accuracy and Relevance â€” 8-agent pipeline, 9 exam families, solution maps directly to the challenge scenario.

> Reasoning and Multi-step Thinking â€” Planner-Executor pattern across the pipeline, 17-rule deterministic Critic, two HITL gates, and a self-reflection remediation loop.

> Creativity and Originality â€” Largest Remainder hour allocation, 3-tier Azure fallback chain, per-exam-family domain weight tables, and guardrail-protected Microsoft Learn curation.

> Reliability and Safety â€” 352 tests, azure-ai-evaluation grounding metrics, Azure Content Safety live API, schema evolution guards on every SQLite read.

> CertPrep does not just generate a study plan. It guides a learner from a paragraph of background text to an evidence-based booking decision â€” and it knows when to say 'not yet'."

**[END on Recommendations tab â€” CONDITIONAL GO verdict visible]**

---

---

## RECORDING QUICK REFERENCE

| Timestamp | Screen | Key action |
|-----------|--------|-----------|
| 0:00â€“0:45 | Login page | Speak architecture â€” do not type yet |
| 0:45 | Click Novice card | 2s pause for form auto-fill |
| 1:00 | Click Generate Plan | Let spinner show "Calling Azure AI Foundry" fully |
| 1:30 | Results KPI cards | 3s pause â€” animation visible |
| 1:45 | Domain Map tab | Hover red risk domains |
| 2:10 | Study Setup tab | Scroll Gantt slowly |
| 2:40 | Learning Path tab | Show module cards |
| 3:00 | Progress tab | Set Gen AI slider = 2 visibly |
| 3:30 | Submit readiness | Pause on gauge animation |
| 3:50 | Quiz tab | Generate â†’ answer 3 â†’ submit |
| 4:30 | Recommendations tab | Scroll to next-cert cards |
| 5:00 | Back to intake | Type SSN + cheat phrase slowly |
| 5:30 | Admin Dashboard tab | Pre-opened, switch instantly |
| 6:00 | Sidebar Mock toggle | Show badge colour change |

---

## KEY PHRASES TO LAND

- *"No downstream agent ever re-reads the original text â€” the profiler produces a plan, the executors carry it out."*
- *"17 rules. BLOCK stops the pipeline. WARN continues with a flag. Deterministic, not LLM-judged."*
- *"Self-rating overwrites the LLM's prior. Real posterior evidence replaces the model's estimate."*
- *"If the quiz score is below 60%, the agents self-correct â€” not the user."*
- *"352 tests. Zero Azure credentials. Under one second. Same pipeline."*

---

*Script version: 3.0 Â· Last updated: 2026-02-26 Â· Runtime: 5â€“7 min Â· Judge-ready*
